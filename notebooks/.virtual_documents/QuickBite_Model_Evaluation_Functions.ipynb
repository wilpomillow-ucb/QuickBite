


import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
import warnings
warnings.filterwarnings("ignore")
import os
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import LabelBinarizer
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score





yolo_training_log_directory = r"C:\Users\linay\Downloads\QuickBite\data\results\Yolov8_Training_Log.csv" 
deit_training_log_directory = r"C:\Users\linay\Downloads\QuickBite\data\results\deit_training_log.csv"


yolo_validation_results_directory = r"C:\Users\linay\Downloads\QuickBite\data\results\Yolov8_Validation_Results_Raw.csv"
deit_validation_results_directory = r"C:\Users\linay\Downloads\QuickBite\data\results\deit_validation_results.csv"
gpt_4o_mini_validation_results_directory = r"C:\Users\linay\Downloads\QuickBite\data\results\gpt-4o-mini.csv" 





# define function to save scores and number of epochs to pandas dataframe for each model's training log
def save_scores_to_df(directory, epochs, train_loss, val_loss, val_accuracy_top1, val_accuracy_top5):
    '''
    stores train and validation loss and accuracy into a pandas dataframe
    '''
    # read in csv 
    df = pd.read_csv(directory)
    
    # create a dictionary to store the scores
    scores = {
        'Epoch': df[epochs],
        'Train Loss': df[train_loss],
        'Val Loss': df[val_loss],
        'Val Accuracy Top 1': df[val_accuracy_top1], 
        'Val Accuracy Top 5': df[val_accuracy_top5] 
    }

    # convert the dictionary to a DataFrame
    df = pd.DataFrame(scores)
    return df 

# call function
deit_training_log_df = save_scores_to_df(deit_training_log_directory, 'Epoch', 'Train/Loss', 'Val/Loss', 
                      'Val_Metrics/Accuracy_Top1',	'Val_Metrics/Accuracy_Top5') 
yolo_training_log_df = save_scores_to_df(yolo_training_log_directory, 'epoch', 'train/loss', 'val/loss', 
                               'metrics/accuracy_top1', 'metrics/accuracy_top5')


# define a function to save validation results into a pandas dataframe
def save_validation_results_to_df(directory, img_name, predicted_class, true_class, confidence_value=None): 
    # read in csv
    raw_df = pd.read_csv(directory) 
    #  because GPT4 model has predicted labels that are unknown, we are going to remove them 
    print('Number of nulls', raw_df.isnull().sum().sum())
    raw_df.dropna(inplace=True)
    
    # create a dictionary to store the validation results 
    validation_results = {
        'Image Name': raw_df[img_name], 
        'Predicted Class': raw_df[predicted_class], 
        'True Class': raw_df[true_class]
    }

    # check if 'confidence_value' column already exists
    if confidence_value is not None:
        validation_results['Confidence Value'] = raw_df[confidence_value]   
    
    # convert dictionary to a DataFrame
    df = pd.DataFrame(validation_results)
    
    return df 

# call function 
yolo_val_results_df = save_validation_results_to_df(yolo_validation_results_directory, 'img_name', 
                                                    'most_confident_label', 'class_name', 
                                                    'most_confident_value') 
deit_val_results_df = save_validation_results_to_df(deit_validation_results_directory, 'img_name',
                                                    'predicted_class', 'true_class', 
                                                    'confidence') 
gpt_4o_mini_val_results_df = save_validation_results_to_df(gpt_4o_mini_validation_results_directory, 'img_name',
                                                    'predicted_class', 'true_class') 


gpt_4o_mini_val_results_df.head(1)


deit_val_results_df.head(1)


yolo_val_results_df.head(1)


gpt_4o_mini_val_results_df.shape


# For GPT
def summary_metrics_df_gpt(df, model): 
    df['True Label'] = pd.Categorical(gpt_4o_mini_val_results_df['True Class']).codes
    df['Predicted Label'] = pd.Categorical(gpt_4o_mini_val_results_df['Predicted Class']).codes
    
    # calculate AUC-ROC
    # first, we need to binarize the labels
    n_classes = len(df['True Label'].unique())
    y = label_binarize(df['True Label'], classes=np.arange(n_classes))
    y_pred = label_binarize(df['Predicted Label'], classes=np.arange(n_classes))

    summary_metrics = { 
        'Accuracy': [accuracy_score(df['True Label'], df['Predicted Label'])],
        'Precision': [precision_score(df['True Label'], df['Predicted Label'], average='macro')],
        'Recall': [recall_score(df['True Label'], df['Predicted Label'], average='macro', zero_division=0)],
        'F1': [f1_score(df['True Label'], df['Predicted Label'], average='macro')],
        'AUC-ROC': [roc_auc_score(y, y_pred, average='macro')]
    }
    # convert the dictionary to a DataFrame
    df = pd.DataFrame(summary_metrics)
    df['model_name'] = model
    return df


# define a function to save Summary Metrics into a pandas dataframe
def summary_metrics_to_df(df, model):
    df['True Label'] = pd.Categorical(df['True Class']).codes
    df['Predicted Label'] = pd.Categorical(df['Predicted Class']).codes

    # auc roc score
    lb = LabelBinarizer()
    y_true = lb.fit_transform(df['True Label'])
    y_pred_proba = np.array([df['Confidence Value']]*y_true.shape[1]).T
    
    # Calculate the weighted AUC-ROC score
    auc_roc_weighted = roc_auc_score(y_true, y_pred_proba, average='weighted')

    # # CAN DELETE IF WE WANT: Calculate the roc_auc_score for each class
    # for i in range(y_true.shape[1]):
    #     auc_roc = roc_auc_score(y_true[:, i], validation_results_df['confidence'])
    #     print(f"AUC-ROC for class {i}: {auc_roc}")

    # Get the top 3 predicted labels
    top_3_predicted_labels = []
    for i in range(len(df['Confidence Value'])):
        top_3_indices = np.argsort(df['Confidence Value'].iloc[i])[-3:][::-1]
        top_3_predicted_labels.append(np.array([df['Predicted Label'].iloc[i]]))
    
    # Check if true label is in top 3 predicted labels
    correct_top_3 = [x in y for x, y in zip(df['True Label'], top_3_predicted_labels)]
    
    # Calculate the top 3 accuracy
    top_3_accuracy = np.mean(correct_top_3)
        
    summary_metrics = {
        'Accuracy': [accuracy_score(df['True Label'], df['Predicted Label'])],
        'Precision': [precision_score(df['True Label'], df['Predicted Label'], average='macro')],
       'Recall': [recall_score(df['True Label'], df['Predicted Label'], average='macro', zero_division=0)],
        'F1': [f1_score(df['True Label'], df['Predicted Label'], average='macro', zero_division=0)],
        'AUC-ROC': auc_roc_weighted, 
        'Top 3 Accuracy': top_3_accuracy
    }
    # convert the dictionary to a DataFrame
    df = pd.DataFrame(summary_metrics)
    df['model_name'] = model
    return df


# call functions for summary metrics and union output 
summary_metrics_df = pd.concat([summary_metrics_to_df(yolo_val_results_df, 'YOLOv8'), 
                                summary_metrics_to_df(deit_val_results_df, 'DeiT'),
                                summary_metrics_df_gpt(gpt_4o_mini_val_results_df, 'GPT 4o')]) 





summary_metrics_df


# Plots the Summary Metrics
# Melt the DataFrame to convert it into a long format
summary_metrics_df_melt = summary_metrics_df.melt(id_vars='model_name')

# Plot the summary metrics
fig, ax = plt.subplots(figsize=(10,6))
sns.barplot(x='variable', y='value', hue='model_name', data=summary_metrics_df_melt, ax=ax)
plt.title('Summary Metrics Comparison')
plt.xlabel('Metric')
plt.ylabel('Value (%)')
plt.legend(title='Model Name', loc='upper right', bbox_to_anchor=(1.15, 1.014))
ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:.0f}%".format(x*100)))

# Add value labels to each bar
for p in ax.patches:
    if p.get_height() > 0:
        ax.text(p.get_x() + p.get_width()/2., p.get_height(), "{:.0f}%".format(p.get_height()*100), ha="center", va="bottom", color="black")

plt.show()


# Plots validation accuracy of each model against each other 
plt.figure(figsize=(15, 15))

# plot Validation Accuracy Top 1
plt.subplot(3, 2, 1)
plt.plot(yolo_training_log_df['Epoch'], yolo_training_log_df['Val Accuracy Top 1'], label='YOLOv8 Val Accuracy Top 1')
plt.fill_between(yolo_training_log_df['Epoch'], yolo_training_log_df['Val Accuracy Top 1'] - 0.05, yolo_training_log_df['Val Accuracy Top 1'] + 0.05, alpha=0.2, label='YOLOv8 Confidence Interval')
plt.plot(deit_training_log_df['Epoch'], deit_training_log_df['Val Accuracy Top 1'], label='DeiT Val Accuracy Top 1')
plt.fill_between(deit_training_log_df['Epoch'], deit_training_log_df['Val Accuracy Top 1'] - 0.05, deit_training_log_df['Val Accuracy Top 1'] + 0.05, alpha=0.2, label='DeiT Confidence Interval')
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy Top 1')
plt.legend()
# plt.xticks(range(1, len(deit_training_log_df['Epoch']) + 1))

# plot Validation Accuracy Top 5 
plt.subplot(3, 2, 2)
plt.plot(yolo_training_log_df['Epoch'], yolo_training_log_df['Val Accuracy Top 5'], label='YOLOv8 Val Accuracy Top 5')
plt.plot(deit_training_log_df['Epoch'], deit_training_log_df['Val Accuracy Top 5'], label='DeiT Val Accuracy Top 15')
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy Top 5')
plt.legend()

# plot Validation Loss
plt.subplot(3, 2, 3) 
plt.plot(yolo_training_log_df['Epoch'], yolo_training_log_df['Val Loss'], label='YOLOv8 Val Loss')
plt.plot(deit_training_log_df['Epoch'], deit_training_log_df['Val Loss'], label='DeiT Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.legend()

# plot Train Loss
plt.subplot(3, 2, 4) 
plt.plot(yolo_training_log_df['Epoch'], yolo_training_log_df['Train Loss'], label='YOLOv8 Val Loss')
plt.plot(deit_training_log_df['Epoch'], deit_training_log_df['Train Loss'], label='DeiT Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Train Loss')
plt.legend()

plt.suptitle('Scores of YOLOv8 and DeiT Models by Epochs', y=0.90)
plt.show()


# Create function to create normalized confusion matrix: 
def create_normy_confusion_matrix(df): 
    # Create a confusion matrix using the confidence values
    cm = np.zeros((len(np.unique(df['True Class'])), len(np.unique(df['Predicted Class']))))
    
    for i, true_class in enumerate(np.unique(df['True Class'])):
        for j, predicted_class in enumerate(np.unique(df['Predicted Class'])):
            cm[i, j] = df[(df['True Class'] == true_class) & (df['Predicted Class'] == predicted_class)]['Confidence Value'].sum()
    
    # Normalize the confusion matrix by dividing by the sum of each row
    normalized_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # Multiply by 100 to convert to percentages
    normalized_cm = normalized_cm * 100

    return normalized_cm 

# Create a figure with two subplots
fig, axs = plt.subplots(1, 3, figsize=(30, 10), sharey='row')
plt.subplots_adjust(wspace=0.01, hspace=0.1)

# create confusion matrix
yolo_cm = create_normy_confusion_matrix(yolo_val_results_df)
deit_cm = create_normy_confusion_matrix(deit_val_results_df)
gpt_cm = confusion_matrix(gpt_4o_mini_val_results_df['True Class'], gpt_4o_mini_val_results_df['Predicted Class'])
gpt_cm = gpt_cm / gpt_cm.sum(axis=1, keepdims=True) * 100

# Create the Yolo heatmap
axs[0].imshow(yolo_cm, interpolation='nearest', cmap='Blues')
axs[0].set_xlabel("Predicted labels")
axs[0].set_ylabel("True labels")
axs[0].set_title("Confusion Matrix Yolov8")
axs[0].set_xticks(range(len(np.unique(yolo_val_results_df['Predicted Class']))), np.unique(yolo_val_results_df['Predicted Class']), rotation=90, fontsize=8)
axs[0].set_yticks(range(len(np.unique(yolo_val_results_df['True Class']))), np.unique(yolo_val_results_df['True Class']), fontsize=8)
# Add text to each square of the first heatmap
for i in range(yolo_cm.shape[0]):
    for j in range(yolo_cm.shape[1]):
        color = plt.cm.Blues(yolo_cm[i, j] / 100)
        if max(color[:3]) < 0.8:
            axs[0].text(j, i, f"{round(yolo_cm[i, j])}%", ha="center", va="center", color="white", fontsize=8)
        else:
            axs[0].text(j, i, f"{round(yolo_cm[i, j])}%", ha="center", va="center", color="black", fontsize=8)

# Create the DeiT heatmap 
axs[1].imshow(deit_cm, interpolation='nearest', cmap='Blues')
axs[1].set_xlabel("Predicted labels")
# axs[1].set_ylabel("True labels")
axs[1].set_title("Confusion Matrix DeiT")
axs[1].set_xticks(range(len(np.unique(deit_val_results_df['Predicted Class']))), np.unique(deit_val_results_df['Predicted Class']), rotation=90, fontsize=8)
# axs[1].set_yticks(range(len(np.unique(deit_val_results_df['True Class']))), np.unique(deit_val_results_df['True Class']), fontsize=8)
# Add text to each square of the second heatmap
for i in range(deit_cm.shape[0]):
    for j in range(deit_cm.shape[1]):
        color = plt.cm.Blues(deit_cm[i, j] / 100)
        if max(color[:3]) < 0.9:
            axs[1].text(j, i, f"{round(deit_cm[i, j])}%", ha="center", va="center", color="white", fontsize=8)
        else:
            axs[1].text(j, i, f"{round(deit_cm[i, j])}%", ha="center", va="center", color="black", fontsize=8)

# Create the GPT 4o heatmap 
axs[2].imshow(gpt_cm, interpolation='nearest', cmap='Blues')
axs[2].set_xlabel("Predicted labels")
# axs[2].set_ylabel("True labels")
axs[2].set_title("Confusion Matrix GPT 4o")
axs[2].set_xticks(range(len(np.unique(gpt_4o_mini_val_results_df['Predicted Class']))), np.unique(gpt_4o_mini_val_results_df['Predicted Class']), rotation=90, fontsize=8)
# axs[2].set_yticks(range(len(np.unique(gpt_4o_mini_val_results_df['True Class']))), np.unique(gpt_4o_mini_val_results_df['True Class']), fontsize=8)
# Add text to each square of the third heatmap
for i in range(gpt_cm.shape[0]):
    for j in range(gpt_cm.shape[1]):
        color = plt.cm.Blues(gpt_cm[i, j] / 100)
        if max(color[:3]) < 0.9:
            axs[2].text(j, i, f"{round(gpt_cm[i, j])}%", ha="center", va="center", color="white", fontsize=8)
        else:
            axs[2].text(j, i, f"{round(gpt_cm[i, j])}%", ha="center", va="center", color="black", fontsize=8)


cbar_ax = fig.add_axes([0.91, 0.1, 0.02, 0.8])
plt.colorbar(axs[0].images[0], cax=cbar_ax)

# plt.savefig(r'C:\Users\linay\Downloads\QuickBite\data\results\confusion_matrices_normalized.png', bbox_inches='tight', pad_inches=0.5, dpi=72, transparent=True)


# Create a function to extract the diagonal values from the confusion matrix
def create_diagonal_df(df, conf_mat, model):
    # get the unique true class labels
    true_class_labels = np.unique(df['True Class'])
    diagonal_values = np.diag(conf_mat)
    
    # get the unique true class labels in the same order as the diagonal values
    true_class_labels = np.unique(df['True Class'])
    true_class_labels = true_class_labels[np.argsort(np.unique(df['True Class']))]
    
    # create a new DataFrame
    diagonal_df = pd.DataFrame({'Food': true_class_labels, 'Diagonal Values': diagonal_values})

    # append model name
    diagonal_df['Model Name'] = model
    
    return diagonal_df


# Get the diagonal scores for GPT
conf_mat = confusion_matrix(gpt_4o_mini_val_results_df['True Class'], gpt_4o_mini_val_results_df['Predicted Class'])
conf_mat = conf_mat / conf_mat.sum(axis=1, keepdims=True) * 100
diagonal_values = np.diag(conf_mat)

# get the unique true class labels in the same order as the diagonal values
true_class_labels = np.unique(gpt_4o_mini_val_results_df['True Class'])
true_class_labels = true_class_labels[np.argsort(np.unique(gpt_4o_mini_val_results_df['True Class']))]

# create a new DataFrame
gpt_diagonal_df = pd.DataFrame({'Food': true_class_labels, 'Diagonal Values': diagonal_values})
gpt_diagonal_df['Model Name'] = 'GPT 4o'

# concat all diagonal scores of models together into 1 dataframe
dot_plot_df = pd.concat([create_diagonal_df(yolo_val_results_df, yolo_cm, 'YOLOv8'),
                            create_diagonal_df(deit_val_results_df, deit_cm, 'DeiT'),
                            gpt_diagonal_df])


sns.set_style("whitegrid")
sns.set_style()
cmap = cm.get_cmap('RdYlGn')

# Define the custom order
model_order = ['YOLOv8', 'DeiT', 'GPT 4o']

# Get the groups in the custom order
groups = [dot_plot_df[dot_plot_df['Model Name'] == name] for name in model_order]

fig, axs = plt.subplots(1, len(model_order), figsize=(20, 6))

for i, group in enumerate(groups):
    if len(group) > 0:  # Check if the subset has any rows
        max_val = group['Diagonal Values'].max()
        min_val = group['Diagonal Values'].min()
        norm = colors.Normalize(min_val, max_val)
        sns.barplot(x="Diagonal Values", y="Food", palette=[cmap(norm(x)) for x in group['Diagonal Values']], data=group, ax=axs[i])
        axs[i].set_title(f"Model: {model_order[i]}")
    else:
        print(f"No data for model {model_order[i]}")

plt.tight_layout()
plt.savefig('diagonal_values_bar_plot.png', dpi=300)
plt.show()


# # assuming your DataFrame is named diagonal_df
# plt.figure(figsize=(10,6))
# sns.stripplot(x="Diagonal Values", y="Food", hue="Model Name", data=dot_plot_df, jitter=True)
# plt.title('Dot Plot of Diagonal Values from Confusion Matrix by True Class')
# plt.show()


# sns.set_style()
# sns.set_style("whitegrid")
# sns.catplot(x="Diagonal Values", y="Food", col="Model Name", data=dot_plot_df, kind="bar")


# sns.set_style("whitegrid")
# sns.set_style()
# cmap = cm.get_cmap('RdYlGn')

# for name, group in dot_plot_df.groupby('Model Name'):
#     if len(group) > 0:  # Check if the subset has any rows
#         max_val = group['Diagonal Values'].max()
#         min_val = group['Diagonal Values'].min()
#         norm = colors.Normalize(min_val, max_val)
#         plt.figure()
#         sns.barplot(x="Diagonal Values", y="Food", palette=[cmap(norm(x)) for x in group['Diagonal Values']], data=group)
#         plt.title(f"Model: {name}")
#         plt.tight_layout()
#     else:
#         print(f"No data for model {name}")
        


# sns.set_style("whitegrid")
# sns.set_style()
# cmap = cm.get_cmap('RdYlGn')

# fig, axs = plt.subplots(1, len(dot_plot_df['Model Name'].unique()), figsize=(20, 6))

# for i, (name, group) in enumerate(dot_plot_df.groupby('Model Name')):
#     if len(group) > 0:  # Check if the subset has any rows
#         max_val = group['Diagonal Values'].max()
#         min_val = group['Diagonal Values'].min()
#         norm = colors.Normalize(min_val, max_val)
#         sns.barplot(x="Diagonal Values", y="Food", palette=[cmap(norm(x)) for x in group['Diagonal Values']], data=group, ax=axs[i])
#         axs[i].set_title(f"Model: {name}")
#     else:
#         print(f"No data for model {name}")

# plt.tight_layout()
# plt.show()
